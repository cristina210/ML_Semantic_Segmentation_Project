{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aDDRsr5ZN2Cr"},"outputs":[],"source":["!pip install ptflops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UdeUazeGEZo"},"outputs":[],"source":["!pip install albumentations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HnkmGHxhaRU"},"outputs":[],"source":["!pip install -U fvcore"]},{"cell_type":"markdown","source":["Image preprocessing for training and validation datasets.\n","The transformations include:\n","1. Resizing all input images to 512x1024 pixels.\n","2. Normalizing pixel values using ImageNet statistics\n"," (mean and standard deviation per channel).\n","3. Converting images to PyTorch tensors"],"metadata":{"id":"fez2g25taIMR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdIvsUFmO538"},"outputs":[],"source":["## Transform ##\n","\n","train_transform = A.Compose([ A.Resize(height=512, width=1024),A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()\n","])\n","\n","val_transform = A.Compose([A.Resize(height=512, width=1024), A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()\n","])"]},{"cell_type":"markdown","source":["Set Up and Environment"],"metadata":{"id":"KJsppdEWYbRz"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40387,"status":"ok","timestamp":1756913163327,"user":{"displayName":"GruppoBIxDB","userId":"02526103203990742405"},"user_tz":-120},"id":"_SbukbYC5aNP","outputId":"040cdb32-5d80-40a1-d154-ce96386c9d9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive not mounted, so nothing to flush and unmount.\n","Mounted at /content/drive\n"]}],"source":["## Import and setup ##\n","import os, time, zipfile, gc, glob\n","import numpy as np\n","from PIL import Image\n","from collections import deque\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from torchvision import transforms\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","from ptflops import get_model_complexity_info\n","from google.colab import drive\n","from torchvision import models\n","import warnings\n","\n","from fvcore.nn import FlopCountAnalysis, flop_count_table\n","\n","\n","drive.flush_and_unmount()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","drive.mount(\"/content/drive\", force_remount=True)\n"]},{"cell_type":"markdown","source":["Definition Bisenet model with backbone ResNet18 o ResNet101"],"metadata":{"id":"azcywg8VZdYC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_FAEqGA5lw3"},"outputs":[],"source":["## Bisenet model with backbone ResNet18 or ResNet101 ##\n","\n","class resnet18(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet18(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)\n","        feature2 = self.layer2(feature1)\n","        feature3 = self.layer3(feature2)\n","        feature4 = self.layer4(feature3)\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","class resnet101(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet101(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)\n","        feature2 = self.layer2(feature1)\n","        feature3 = self.layer3(feature2)\n","        feature4 = self.layer4(feature3)\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","def build_contextpath(name):\n","    model = {\n","        'resnet18': resnet18(pretrained=True),\n","        'resnet101': resnet101(pretrained=True)\n","    }\n","    return model[name]\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n","                               stride=stride, padding=padding, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        return self.relu(self.bn(x))\n","\n","\n","class Spatial_path(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.convblock1 = ConvBlock(3, 64)\n","        self.convblock2 = ConvBlock(64, 128)\n","        self.convblock3 = ConvBlock(128, 256)\n","\n","    def forward(self, input):\n","        x = self.convblock1(input)\n","        x = self.convblock2(x)\n","        x = self.convblock3(x)\n","        return x\n","\n","\n","class AttentionRefinementModule(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.in_channels = in_channels # Add this line\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.sigmoid = nn.Sigmoid()\n","        self.in_channels = in_channels\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input):\n","        # global average pooling\n","        x = self.avgpool(input)\n","        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n","        x = self.conv(x)\n","        x = self.sigmoid(self.bn(x))\n","        # x = self.sigmoid(x)\n","        # channels of input and x should be same\n","        return torch.mul(input, x)\n","\n","\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self, num_classes, in_channels):\n","        super().__init__()\n","        # self.in_channels = input_1.channels + input_2.channels\n","        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n","        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n","        self.in_channels = in_channels\n","\n","        self.convblock = ConvBlock(in_channels, num_classes, stride=1)\n","        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input_1, input_2):\n","        x = torch.cat((input_1, input_2), dim=1)\n","        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n","        feature = self.convblock(x)\n","\n","        x = self.avgpool(feature)\n","        x = self.relu(self.conv1(x))\n","        x = self.sigmoid(self.conv2(x))\n","        x = torch.mul(feature, x)\n","        return torch.add(x, feature)\n","\n","\n","class BiSeNet(nn.Module):\n","    def __init__(self, num_classes, context_path='resnet18'):\n","        super().__init__()\n","        # build spatial path\n","        self.saptial_path = Spatial_path()\n","\n","        # build context path\n","        self.context_path = build_contextpath(context_path)\n","\n","        # build attention refinement module  for resnet 101\n","        if context_path == 'resnet101':\n","            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n","            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(1024, num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(2048, num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n","\n","        elif context_path == 'resnet18':\n","             # build attention refinement module  for resnet 18\n","            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n","            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(256, num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(512, num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n","\n","        # build final convolution\n","        self.conv = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        for name, m in self.named_modules():\n","            if 'context_path' not in name:\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n","                elif isinstance(m, nn.BatchNorm2d):\n","                    m.eps = 1e-5\n","                    m.momentum = 0.1\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, input):\n","        # output of spatial path\n","        sx = self.saptial_path(input)\n","\n","        # output of context path\n","        cx1, cx2, tail = self.context_path(input)\n","\n","        cx1 = self.attention_refinement_module1(cx1)\n","        cx2 = self.attention_refinement_module2(cx2)\n","        cx2 = torch.mul(cx2, tail)\n","        # upsampling\n","        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n","        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n","        cx = torch.cat((cx1, cx2), dim=1)\n","\n","        if self.training:\n","            cx1_sup = self.supervision1(cx1)\n","            cx2_sup = self.supervision2(cx2)\n","            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n","            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n","\n","        # output of feature fusion module\n","        result = self.feature_fusion_module(sx, cx)\n","\n","        # upsampling\n","        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n","        result = self.conv(result)\n","\n","        if self.training:\n","            return result, cx1_sup, cx2_sup\n","\n","        return result\n","\n","\n","def get_bisenet_model(num_classes, context_path='resnet18'):\n","    return BiSeNet(num_classes=num_classes, context_path=context_path)"]},{"cell_type":"markdown","source":["Cityscapes Dataset and DataLoader Setup.\n","\n","This section defines a custom PyTorch Dataset class for loading the Cityscapes\n","dataset.\n","DataLoaders are then created for both training and validation."],"metadata":{"id":"qm9ZjdnzZ2Hy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sjTE2R3UYwz"},"outputs":[],"source":["## Dataset Cityscapes ##\n","\n","class CityscapesDataset(Dataset):\n","    def __init__(self, root, split=\"train\", transform=None):\n","        self.transform = transform\n","        self.images = sorted(glob.glob(f\"{root}/images/{split}/**/*.png\", recursive=True))\n","        self.labels = sorted(glob.glob(f\"{root}/gtFine/{split}/**/*_labelTrainIds.png\", recursive=True))\n","\n","        min_lenght = min(len(self.images), len(self.labels))\n","        self.images = self.images[:min_lenght]\n","        self.labels = self.labels[:min_lenght]\n","\n","    def __len__(self):\n","        # Return the number of samples\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = np.array(Image.open(self.images[idx]).convert(\"RGB\"))\n","        label = np.array(Image.open(self.labels[idx]), dtype=np.int64)\n","\n","        if self.transform:\n","            # Apply the training transform to image and mask\n","            trasformation_im_lab = self.transform(image=img, mask=label)\n","            # Extract the transformed image and mask\n","            img = trasformation_im_lab['image']\n","            label = trasformation_im_lab['mask']\n","\n","        # Convert label to LongTensor\n","        if isinstance(label, np.ndarray):\n","            label = torch.from_numpy(label).long()\n","        else:\n","            label = label.long()\n","\n","        return img, label\n","\n","\n","## Create datasets for training and validation ##\n","\n","train_dataset = CityscapesDataset(root=\"/content/drive/MyDrive/Cityscapes/Cityspaces\", split=\"train\", transform=train_transform)\n","\n","val_dataset = CityscapesDataset(root=\"/content/drive/MyDrive/Cityscapes/Cityspaces\",split=\"val\", transform=val_transform)\n","\n","\n","## DataLoader setup ##\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=4,\n","    shuffle=True,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=4,\n","    shuffle=False,     # No shuffle for validation\n","    num_workers=2,\n","    pin_memory=True\n",")\n"]},{"cell_type":"markdown","source":["Defining the function for the learning rate: polynomial learning rate."],"metadata":{"id":"CtFGmPssabXe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuYO-K8cY9__"},"outputs":[],"source":["## learning rate ##\n","\n","def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n","                      max_iter=300, power=0.9):\n","\n","    \"\"\"Polynomial decay of learning rate\n","        :param init_lr is base learning rate\n","        :param iter is a current iteration\n","        :param lr_decay_iter how frequently decay occurs, default is 1\n","        :param max_iter is number of maximum iterations\n","        :param power is a polymomial power\n","      Returns the scalar learning rate\"\"\"\n","\n","\n","    lr = init_lr*(1 - iter/max_iter)**power\n","    optimizer.param_groups[0]['lr'] = lr\n","    return lr"]},{"cell_type":"markdown","source":["\n","This section sets up the DeepLab v2 model with a pre-trained ResNet18 backbone,\n","defines the loss function, and configures the optimizer"],"metadata":{"id":"mCZZDYFuKY81"}},{"cell_type":"code","source":["## Model, loss function, and optimizer ##\n","\n","# # Initialize DeepLab v2 model with pre-trained ResNet18 backbone\n","model = get_bisenet_model(num_classes=19, context_path='resnet18').to(device)\n","\n","# Cross-entropy loss\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n","\n","# SGD optimizer with momentum and weight decay\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.0025, momentum=0.9, weight_decay=5e-4)"],"metadata":{"id":"33HO1Vk2KaP-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Checkpoint Management, Device Setup, and AMP Initialization.\n","\n","This section prepares the training environment by:\n","   - Creating a directory to save model checkpoints\n","   - Selecting the computation device (GPU if available, else CPU).\n","   - Initializing PyTorchâ€™s Automatic Mixed Precision GradScaler"],"metadata":{"id":"e2ruyEDQbiFi"}},{"cell_type":"code","source":["## Setup directories, device, and AMP scaler ##\n","\n","# Directory to save model checkpoints\n","checkpoint_dir = \"/content/drive/MyDrive/2bMachineLearning/checkpoints\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","max_checkpoints = 2\n","saved_checkpoints = deque()\n","\n","\n","# Select device (GPU if available, else CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Automatic Mixed Precision: GradScaler\n","scaler = torch.amp.GradScaler(device='cuda') if device.type == 'cuda' else None\n","\n","## Restore from latest checkpoint if available ##\n","latest_checkpoint = None\n","latest_epoch = -1\n","for fname in os.listdir(checkpoint_dir):\n","    if fname.startswith(\"checkpoint_epoch\") and fname.endswith(\".pt\"):\n","        epoch_num = int(fname.split(\"_epoch\")[1].split(\".\")[0])\n","        if epoch_num > latest_epoch:\n","            latest_epoch = epoch_num\n","            latest_checkpoint = os.path.join(checkpoint_dir, fname)\n","\n","start_epoch = 0\n","if latest_checkpoint:\n","    checkpoint = torch.load(latest_checkpoint, map_location=device, weights_only=False)     # Load checkpoint\n","\n","    # Restore model and optimizer state:\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # Restore AMP scaler if used\n","    if scaler and checkpoint.get('scaler_state_dict'):\n","        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n","\n","    # Resume from next epoch\n","    start_epoch = checkpoint['epoch'] + 1\n","    print(f\"Restored from {latest_checkpoint}\")\n","else:\n","    print(\" No checkpoint found. Starting from scratch\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxIEbaCPtZoa","executionInfo":{"status":"ok","timestamp":1756913733232,"user_tz":-120,"elapsed":191,"user":{"displayName":"GruppoBIxDB","userId":"02526103203990742405"}},"outputId":"53c71d77-a879-45e3-b899-57b89ed52096"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Restored from /content/drive/MyDrive/2bMachineLearning/checkpoints/checkpoint_epoch48.pt\n"]}]},{"cell_type":"markdown","metadata":{"id":"JF2xoH5m85ms"},"source":["This section presents two functions that\n","- Compute mean IoUs per class and mIoU\n","- Measure model latency & FPS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGK17c8b5jb5"},"outputs":[],"source":["def compute_miou(preds, labels, num_classes=19, device=\"cuda\"):\n","    \"\"\"\n","    Compute the mean Intersection over Union (mIoU) for semantic segmentation.\n","\n","    This function calculates the IoU for each class and returns both the mean IoU\n","    and the list of per-class IoUs. It handles the common \"void\" label (255) by\n","    excluding it from false positive calculations.\n","    \"\"\"\n","\n","\n","    # Initialize variables for storing: True Positives, False Positives and False Negatives\n","    tp = torch.zeros(num_classes, dtype=torch.int64, device=device)\n","    fp = torch.zeros(num_classes, dtype=torch.int64, device=device)\n","    fn = torch.zeros(num_classes, dtype=torch.int64, device=device)\n","\n","\n","    # TP, FP, FN for each class\n","    for cls in range(num_classes):\n","        # True Positive\n","        tp[cls] += ((labels == cls) & (preds == cls)).sum()\n","        # False Positive\n","        fp[cls] += ((labels != cls) & (labels != 255) & (preds == cls)).sum()\n","        # False Negative\n","        fn[cls] += ((labels == cls) & (preds != cls)).sum()\n","\n","    iou_per_class = []\n","\n","    # Compute IoU for each class and store in a list\n","    for cls in range(num_classes):\n","        denom = tp[cls] + fp[cls] + fn[cls]\n","        iou = tp[cls].float() / (denom.float() + 1e-10)\n","        print(f\"Class {cls}: TP={tp[cls].item()}, FP={fp[cls].item()}, FN={fn[cls].item()}, IoU={iou.item():.4f}\")\n","        if denom > 0:  # only include classes with at least one pixel\n","            iou_per_class.append(iou.item())\n","\n","    mean_iou = np.mean(iou_per_class) if iou_per_class else 0.0\n","    return mean_iou, iou_per_class\n","\n","\n","def measure_latency_and_fps(model, device, input_size=(3, 512, 1024), iterations=1000):\n","    \"\"\"\n","    Measure inference latency and FPS of a PyTorch model.\n","    This function runs the model multiple times on a random input tensor and computes\n","    the average latency per forward pass and frames per second (FPS).\n","    \"\"\"\n","    model.eval().to(device)\n","\n","    # Create a random tensor\n","    image = torch.randn(1, *input_size).to(device)\n","\n","    latencies = []\n","    fps_list = []\n","\n","    with torch.no_grad():\n","        for _ in range(iterations):\n","            start = time.time()\n","            _ = model(image)    # forward pass\n","            if device == \"cuda\":\n","                torch.cuda.synchronize()\n","            end = time.time()\n","\n","            elapsed = end - start\n","            latencies.append(elapsed)\n","            fps_list.append(1.0 / elapsed)\n","\n","    mean_latency = np.mean(latencies) * 1000\n","    std_latency = np.std(latencies) * 1000\n","    mean_fps = np.mean(fps_list)\n","    std_fps = np.std(fps_list)\n","\n","    print(f\"Latency : {mean_latency:.2f} ms Â± {std_latency:.2f} ms\")\n","    print(f\"FPS: {mean_fps:.2f} Â± {std_fps:.2f}\")\n"]},{"cell_type":"markdown","source":["Training of the model"],"metadata":{"id":"_v_9oPJ9eCpK"}},{"cell_type":"code","source":["\n","## Variables for training progress ##\n","\n","num_epochs = 50    # Number of epochs\n","num_classes = 19    # Number of segmentation classes\n","\n","# Best mIoU, corresponding checkpoint and list for storing evaluation of loss function during epochs\n","best_miou = 0.0\n","best_epoch_ckpt = None\n","epoch_loss_list = []\n","\n","## hyperparameters ##\n","\n","initial_lr = 0.025   # Initial learning rate\n","alpha = 0.4     # Weight coefficient for auxiliary loss\n","\n","scaler = torch.cuda.amp.GradScaler()\n","\n","## training loop ##\n","\n","for epoch in range(start_epoch,num_epochs):\n","    model.train()\n","    total_loss = 0.0\n","    lr = poly_lr_scheduler(optimizer, initial_lr, epoch)\n","\n","    for batch_idx, (images, labels) in enumerate(train_loader):\n","        images = images.to(device, non_blocking=True)\n","        labels = labels.long().to(device,non_blocking=True)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward step\n","        with torch.cuda.amp.autocast():\n","            outputs = model(images)\n","\n","            if isinstance(outputs, (list, tuple)):\n","                main_pred, *aux_preds = outputs\n","                main_pred = F.interpolate(main_pred, size=labels.shape[1:],mode='bilinear', align_corners=False)\n","\n","                # Main loss\n","                loss = criterion(main_pred, labels)\n","\n","                # Auxiliary losses\n","                for aux in aux_preds:\n","                    aux =F.interpolate(aux, size=labels.shape[1:], mode='bilinear', align_corners=False)\n","                    loss += alpha * criterion(aux, labels)\n","            else:\n","                main_pred = F.interpolate(outputs,size=labels.shape[1:], mode='bilinear', align_corners=False)\n","                loss = criterion(main_pred, labels)\n","\n","        # Backward step\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","        # Cleanup GPU\n","        del images,labels, outputs, loss\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","\n","    epoch_loss = total_loss /len(train_loader)\n","    epoch_loss_list.append(epoch_loss)\n","\n","    ## Miou on validation set ##\n","\n","    all_predictions =[]\n","    all_labels = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device, non_blocking=True)\n","            labels= labels.long().to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            if isinstance(outputs, (list,tuple)):\n","                main_pred = outputs[0]\n","            else:\n","                main_pred = outputs\n","\n","            main_pred = F.interpolate(main_pred, size=labels.shape[1:], mode=\"bilinear\", align_corners=False)\n","\n","            probabilities = torch.nn.functional.softmax(main_pred, dim=1)\n","            predictions = torch.argmax(probabilities,dim=1)\n","\n","            all_predictions.append(predictions)\n","            all_labels.append(labels)\n","\n","    all_predictions = torch.cat(all_predictions, dim=0).cpu()\n","    all_labels =torch.cat(all_labels, dim=0).cpu()\n","\n","    epoch_miou, IoU_per_class = compute_miou(all_predictions, all_labels, num_classes=num_classes)\n","    print(f\"ðŸ“Š End epoch {epoch+1} â€” Loss: {epoch_loss:.4f}, mIoU: {epoch_miou:.4f}, LR at the end of epoch: {lr:.6f}\")\n","\n","\n","    ## Saving checkpoints every 3 epochs ##\n","    if (epoch + 1) % 3 == 0:\n","        checkpoint_filename = f\"checkpoint_epoch{epoch+1}.pt\"\n","        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n","\n","        # Save model, optimizer, scaler, loss, and mIoU\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scaler_state_dict':scaler.state_dict(),\n","            'loss': epoch_loss,\n","            'miou': epoch_miou,\n","        }, checkpoint_path)\n","        saved_checkpoints.append(checkpoint_path)\n","        print(f\"Checkpoint saved: {checkpoint_filename}\")\n","\n","        while len(saved_checkpoints) > max_checkpoints:\n","            old_ckpt =saved_checkpoints.popleft()\n","            if os.path.exists(old_ckpt):\n","                os.remove(old_ckpt)\n","                print(f\"Removed old checkpoint: {os.path.basename(old_ckpt)}\")\n","\n","    ## Saving the best model ##\n","    if epoch_miou > best_miou:\n","        best_miou =epoch_miou\n","        best_epoch_ckpt = os.path.join(checkpoint_dir, \"2b_best_epoch.pt\")\n","\n","        # Save model, optimizer, scaler, loss, and mIoU as the best checkpoin\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict':model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scaler_state_dict': scaler.state_dict(),\n","            'loss': epoch_loss,\n","            'miou': epoch_miou,\n","        }, best_epoch_ckpt)\n","        print(f\"New best model saved:{best_epoch_ckpt} con mIoU {best_miou:.4f}\")\n","\n","\n","print(f\"epoch_loss_list: {epoch_loss_list}\")\n","\n","\n","## FLOPs, latency e FPS on the trained model ##\n","model.eval()\n","image = torch.zeros((1, 3, 512, 1024)).to(device)\n","\n","# Compute FLOPs for the model\n","flops = FlopCountAnalysis(model, image)\n","print(flop_count_table(flops))\n","\n","# Measure model latency and FPS\n","measure_latency_and_fps(model, device=device)"],"metadata":{"id":"NaovdL9MjV9x"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}