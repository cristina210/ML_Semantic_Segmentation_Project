{"cells":[{"cell_type":"code","source":["## Import and setup ##\n","\n","from google.colab import drive\n","import os, gc, time, random, glob, warnings\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF\n","from PIL import Image, ImageFilter\n","import numpy as np\n","from collections import deque\n","import matplotlib.pyplot as plt\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","drive.mount('/content/drive')\n","base_dir = '/content/drive/MyDrive/GTA5/GTA5'"],"metadata":{"id":"Djp0_ljOLXhp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definition Bisenet model with backbone ResNet"],"metadata":{"id":"UMgVbcVpi9uo"}},{"cell_type":"code","source":["## Bisenet model with backbone ResNet18 or ResNet101 ##\n","\n","class resnet18(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet18(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)\n","        feature2 = self.layer2(feature1)\n","        feature3 = self.layer3(feature2)\n","        feature4 = self.layer4(feature3)\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","class resnet101(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet101(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)\n","        feature2 = self.layer2(feature1)\n","        feature3 = self.layer3(feature2)\n","        feature4 = self.layer4(feature3)\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","def build_contextpath(name):\n","    model = {\n","        'resnet18': resnet18(pretrained=True),\n","        'resnet101': resnet101(pretrained=True)\n","    }\n","    return model[name]\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n","                               stride=stride, padding=padding, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        return self.relu(self.bn(x))\n","\n","\n","class Spatial_path(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.convblock1 = ConvBlock(3, 64)\n","        self.convblock2 = ConvBlock(64, 128)\n","        self.convblock3 = ConvBlock(128, 256)\n","\n","    def forward(self, input):\n","        x = self.convblock1(input)\n","        x = self.convblock2(x)\n","        x = self.convblock3(x)\n","        return x\n","\n","\n","class AttentionRefinementModule(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.sigmoid = nn.Sigmoid()\n","        self.in_channels = in_channels\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input):\n","        # global average pooling\n","        x = self.avgpool(input)\n","        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n","        x = self.conv(x)\n","        x = self.sigmoid(self.bn(x))\n","        # x = self.sigmoid(x)\n","        # channels of input and x should be same\n","        x = torch.mul(input, x)\n","        return x\n","\n","\n","class FeatureFusionModule(torch.nn.Module):\n","    def __init__(self, num_classes, in_channels):\n","        super().__init__()\n","        # self.in_channels = input_1.channels + input_2.channels\n","        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n","        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n","        self.in_channels = in_channels\n","\n","        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n","        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input_1, input_2):\n","        x = torch.cat((input_1, input_2), dim=1)\n","        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n","        feature = self.convblock(x)\n","        x = self.avgpool(feature)\n","\n","        x = self.relu(self.conv1(x))\n","        x = self.sigmoid(self.conv2(x))\n","        x = torch.mul(feature, x)\n","        x = torch.add(x, feature)\n","        return x\n","\n","\n","class BiSeNet(nn.Module):\n","    def __init__(self, num_classes, context_path='resnet18'):\n","        super().__init__()\n","        self.spatial_path = Spatial_path()\n","        self.context_path = build_contextpath(context_path)\n","\n","        if context_path == 'resnet101':\n","            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n","            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n","            self.supervision1 = nn.Conv2d(1024, num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(2048, num_classes, kernel_size=1)\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n","        elif context_path == 'resnet18':\n","            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n","            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n","            self.supervision1 = nn.Conv2d(256, num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(512, num_classes, kernel_size=1)\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n","\n","        self.conv = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.init_weight()\n","\n","        self.mul_lr = []\n","        self.mul_lr.append(self.spatial_path)\n","        #self.mul_lr.append(self.saptial_path)\n","        self.mul_lr.append(self.attention_refinement_module1)\n","        self.mul_lr.append(self.attention_refinement_module2)\n","        self.mul_lr.append(self.supervision1)\n","        self.mul_lr.append(self.supervision2)\n","        self.mul_lr.append(self.feature_fusion_module)\n","        self.mul_lr.append(self.conv)\n","\n","\n","    def init_weight(self):\n","        for name, m in self.named_modules():\n","            if 'context_path' not in name:\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n","                elif isinstance(m, nn.BatchNorm2d):\n","                    m.eps = 1e-5\n","                    m.momentum = 0.1\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, input):\n","        # output of spatial path\n","        sx = self.spatial_path(input)\n","\n","        # output of context path\n","        cx1, cx2, tail = self.context_path(input)\n","        cx1 = self.attention_refinement_module1(cx1)\n","        cx2 = self.attention_refinement_module2(cx2)\n","        cx2 = torch.mul(cx2, tail)\n","        # upsampling\n","        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n","        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n","        cx = torch.cat((cx1, cx2), dim=1)\n","\n","        if self.training == True:\n","            cx1_sup = self.supervision1(cx1)\n","            cx2_sup = self.supervision2(cx2)\n","            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n","            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n","\n","        # output of feature fusion module\n","        result = self.feature_fusion_module(sx, cx)\n","\n","        # upsampling\n","        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n","        result = self.conv(result)\n","\n","        if self.training == True:\n","            return result, cx1_sup, cx2_sup\n","\n","        return result\n"],"metadata":{"id":"JnrejhAWXmnZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definition of transformations used as preprocessing and data augmentation"],"metadata":{"id":"olkT0ADgMClP"}},{"cell_type":"code","source":["## Class for transformation ##\n","\n","class SegmentationTransform:\n","    \"\"\"\n","    Class to apply preprocessing and data augmentation.\n","    As data augmentation it includes horizontal flipping, color jitter and Gaussian blur;\n","    each of them applied with probability of 0.5.\n","    \"\"\"\n","    def __init__(self, resize, flip = False , gaussian_blur=False, color_jitter=False, train=True):\n","\n","        # Resize for images and masks\n","        self.resize_mask = transforms.Resize(resize, interpolation=Image.NEAREST)\n","        self.resize_img = transforms.Resize(resize,interpolation=Image.BILINEAR)\n","\n","        # Augmentations\n","        self.flip_flag =flip and train\n","        self.color_jitter_flag = color_jitter and train\n","        self.gaussian_blur_flag = gaussian_blur and train\n","        self.color_jitter = transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)\n","\n","        # ImageNet normalization\n","        self.mean = [0.485,0.456, 0.406]\n","        self.std  =[0.229, 0.224, 0.225]\n","\n","    def __call__(self, img, mask):\n","\n","        # Resize\n","        img  = self.resize_img(img)\n","        mask = Image.fromarray(mask.astype(np.uint8))\n","        mask = self.resize_mask(mask)\n","\n","        # Horizontal flip\n","        if self.flip_flag and random.random() < 0.5:\n","            img = TF.hflip(img)\n","            mask = TF.hflip(mask)\n","\n","        # color jitter\n","        if self.color_jitter_flag and random.random() < 0.5:\n","            img=self.color_jitter(img)\n","\n","        # gaussian blur\n","        if self.gaussian_blur_flag and random.random() < 0.5:\n","            img = img.filter(ImageFilter.GaussianBlur(radius=1.5))\n","\n","        # conversion to tensor and normalize\n","        img  = TF.to_tensor(img)\n","        img  = TF.normalize(img, mean=self.mean, std=self.std)\n","        mask = torch.from_numpy(np.array(mask, dtype=np.uint8)).long()\n","\n","        return img, mask\n","\n"],"metadata":{"id":"01y7hUN0XqH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Apply transformation ##\n","\n","# Baseline (no augmentation)\n","transform_gta5_baseline = SegmentationTransform(resize=(720,1280), train=True)\n","\n","# Horizontal flip only\n","transform_gta5_aug1 = SegmentationTransform(resize=(720,1280), flip=True, color_jitter=False,gaussian_blur=False, train=True)\n","\n","# Color jitter only\n","transform_gta5_aug2 = SegmentationTransform(resize=(720,1280), flip =False, color_jitter=True, gaussian_blur=False, train=True)\n","\n","# Gaussian blur only\n","transform_gta5_aug3 = SegmentationTransform(resize=(720,1280), flip =False, color_jitter=False, gaussian_blur=True, train=True)\n","\n","# All augmentations combined\n","transform_gta5_allaug = SegmentationTransform(resize=(720,1280), flip =True, color_jitter=True, gaussian_blur=True, train=True)\n","\n","# Horizontal flip + Color jitter\n","transform_gta5_1_2  = SegmentationTransform(resize=(720,1280), flip =True, color_jitter=True, gaussian_blur=False, train=True)\n","\n","# Color jitter + Gaussian blur\n","transform_gta5_3_2  = SegmentationTransform(resize=(720,1280), flip =False, color_jitter=True, gaussian_blur=True, train=True)\n","\n","# Validation Cityscapes without augmentations\n","transform_cityscapes_val = SegmentationTransform(resize=(512,1024), train=False)"],"metadata":{"id":"bDeE2trFjaXa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset GTA5"],"metadata":{"id":"t5VPuXMCMaVX"}},{"cell_type":"code","source":["\n","ID_TO_TRAINID = {\n","    7:0, 8:1, 11:2, 12:3, 13:4, 17:5,\n","    19:6, 20:7, 21:8, 22:9, 23:10, 24:11,\n","    25:12, 26:13, 27:14, 28:15, 31:16, 32:17, 33:18\n","}    # This dictionary is mapping the original label IDs to the corresponding Cityscapes training IDs.\n","\n","def mapping_labels(label_np):\n","    \"\"\"\n","    Map GTA5 label IDs to Cityscapes training IDs.\n","    ID which is not mapped is set to the ignore index 255.\n","    \"\"\"\n","    mapped = np.full_like(label_np, 255)  # Initialize with ignore value\n","    for k, v in ID_TO_TRAINID.items():\n","        mapped[label_np == k] = v\n","    return mapped\n","\n","\n","## Dataset GTA5 ##\n","\n","class GTA5Dataset(Dataset):\n","    \"\"\"\n","    Dataset for GTA5 dataset.\n","    Loads images and labels and transformations.\n","    \"\"\"\n","    def __init__(self, root, transform=None):\n","        self.transform = transform\n","\n","        # Load all images and labels\n","        self.images = sorted(glob.glob(f\"{root}/images/**/*.png\", recursive=True))\n","        self.labels = sorted(glob.glob(f\"{root}/labels/**/*.png\", recursive=True))\n","        min_len = min(len(self.images), len(self.labels))\n","        self.images = self.images[:min_len]\n","        self.labels = self.labels[:min_len]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","\n","        # Load image and label\n","        img = Image.open(self.images[idx]).convert(\"RGB\")\n","        label_np = np.array(Image.open(self.labels[idx]), dtype=np.int64)\n","\n","        # Map GTA5 labels to Cityscapes training IDs\n","        label_mapped = mapping_labels(label_np)\n","\n","        # Optional transformations\n","        if self.transform:\n","            img, label_mapped = self.transform(img, label_mapped)\n","\n","        return img, label_mapped\n","\n"],"metadata":{"id":"F6IRlxZjYNR7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset Cityscapes"],"metadata":{"id":"OhOorbXEMfBN"}},{"cell_type":"code","source":["## Dataset Cityscapes ##\n","\n","class CityscapesDataset(Dataset):\n","\n","    def __init__(self, root, split=\"val\", transform=None):\n","        self.transform = transform\n","\n","        # Load all images and corresponding labelTrainIds masks\n","        self.images = sorted(glob.glob(f\"{root}/images/{split}/**/*.png\", recursive=True))\n","        self.labels = sorted(glob.glob(f\"{root}/gtFine/{split}/**/*_labelTrainIds.png\", recursive=True))\n","        min_len = min(len(self.images), len(self.labels))\n","        self.images = self.images[:min_len]\n","        self.labels = self.labels[:min_len]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Load image and corresponding label\n","        img = Image.open(self.images[idx]).convert(\"RGB\")\n","        label_np = np.array(Image.open(self.labels[idx]), dtype=np.int64)\n","\n","        # Optional transformations\n","        if self.transform:\n","            img, label_np = self.transform(img, label_np)\n","\n","        return img, label_np\n"],"metadata":{"id":"RXBgpp6bYw_h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Datasets and dataLoaders are created for both training and validation."],"metadata":{"id":"01UXiJ-SNtPS"}},{"cell_type":"code","source":["## Create datasets for training and validation ##\n","\n","train_dataset = GTA5Dataset( root=\"/content/drive/MyDrive/GTA5/GTA5_\",transform= transform_gta5_3_2)\n","\n","val_dataset = CityscapesDataset(root=\"/content/drive/MyDrive/Cityscapes/Cityspaces\", split=\"val\", transform=transform_cityscapes_val)\n","\n","\n","## DataLoader setup ##\n","\n","train_loader = DataLoader(train_dataset,\n","                          batch_size=4,\n","                          shuffle=True,\n","                          num_workers=2)\n","\n","val_loader = DataLoader(val_dataset,\n","                        batch_size=4,\n","                        shuffle=False,\n","                        num_workers=2)"],"metadata":{"id":"RZCF2l0mNGoY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function that computes IoUs per class and mIoU"],"metadata":{"id":"T9IVEswoMw_c"}},{"cell_type":"code","source":["def compute_miou(preds, labels, num_classes=19, device=\"cuda\"):\n","    \"\"\"\n","    Compute the mean Intersection over Union (mIoU) for semantic segmentation.\n","\n","    This function calculates the IoU for each class and returns both the mean IoU\n","    and the list of per-class IoUs. It handles the common \"void\" label (255) by\n","    excluding it from false positive calculations.\n","    \"\"\"\n","\n","\n","    # Initialize variable to store True Positives, False Positives and False Negatives\n","    tp = torch.zeros(num_classes,dtype=torch.int64, device=device)\n","    fp = torch.zeros(num_classes,dtype=torch.int64, device=device)\n","    fn = torch.zeros(num_classes,dtype=torch.int64, device=device)\n","\n","\n","    # Compute TP, FP and FN\n","    for cls in range(num_classes):\n","        # True Positive\n","        tp[cls] += ((labels == cls) & (preds == cls)).sum()\n","        # False Positive\n","        fp[cls] += ((labels != cls) & (labels != 255) & (preds == cls)).sum()\n","        # False Negative\n","        fn[cls] += ((labels == cls) & (preds != cls)).sum()\n","\n","    iou_per_class = []\n","\n","    # Compute IoU for each class and store in a list\n","    for cls in range(num_classes):\n","        denom = tp[cls] + fp[cls] +fn[cls]\n","        iou =tp[cls].float() / (denom.float() + 1e-10)\n","        print(f\"Class {cls}: TP={tp[cls].item()}, FP={fp[cls].item()}, FN={fn[cls].item()}, IoU={iou.item():.4f}\")\n","        if denom > 0:  # only include classes with at least one pixel\n","            iou_per_class.append(iou.item())\n","\n","\n","    mean_iou =np.mean(iou_per_class) if iou_per_class else 0.0\n","    return mean_iou, iou_per_class\n"],"metadata":{"id":"gu4C8OcDY4Wd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function for the learning rate: polynomial learning rate."],"metadata":{"id":"0USq4h5ZM-6J"}},{"cell_type":"code","source":["## learning rate ##\n","\n","def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n","                      max_iter=300, power=0.9):\n","\n","    \"\"\"Polynomial decay of learning rate\n","        :param init_lr is base learning rate\n","        :param iter is a current iteration\n","        :param lr_decay_iter how frequently decay occurs, default is 1\n","        :param max_iter is number of maximum iterations\n","        :param power is a polymomial power\n","      Returns the scalar learning rate\"\"\"\n","\n","\n","    lr = init_lr*(1 - iter/max_iter)**power\n","    optimizer.param_groups[0]['lr'] = lr\n","    return lr"],"metadata":{"id":"5yoxuPftNku1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This section sets up the Bisenet model with a pre-trained ResNet backbone,\n","defines the loss function and configures the optimizer."],"metadata":{"id":"zjQt48kkJYHo"}},{"cell_type":"code","source":["## Model, loss function, and optimizer ##\n","\n","# Initialize DeepLab v2 model with pre-trained ResNet18 backbone\n","model = BiSeNet(num_classes=19).to(device)\n","\n","# Cross-entropy loss\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=255)\n","\n","# SGD optimizer with momentum and weight decay\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.0025, momentum=0.9, weight_decay=5e-4)"],"metadata":{"id":"zrbxGk3FJZui"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Checkpoint Management, Device Setup and AMP Initialization.\n","\n","This section prepares the training environment by:\n","   - Creating a directory to save model checkpoints\n","   - Selecting the computation device (GPU if available, else CPU).\n","   - Initializing PyTorch’s Automatic Mixed Precision GradScaler\n","   "],"metadata":{"id":"uiKWwL7EnHeF"}},{"cell_type":"code","source":["## Setup directories, device, and AMP scaler ##\n","\n","# Directory to save model checkpoints\n","checkpoint_dir = \"/content/drive/MyDrive/3MachineLearning/checkpoints\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","max_checkpoints = 2\n","saved_checkpoints = deque()\n","\n","\n","# Automatic Mixed Precision (AMP): GradScaler\n","scaler = torch.amp.GradScaler(device='cuda') if device.type== 'cuda' else None\n","\n","## Restore from latest checkpoint if available ##\n","resume_training = True\n","latest_checkpoint = None\n","latest_epoch = -1\n","\n","for fname in os.listdir(checkpoint_dir):\n","    if fname.startswith(\"checkpoint_epoch\") and fname.endswith(\".pt\"):\n","        epoch_num = int(fname.split(\"_epoch\")[1].split(\".\")[0])\n","        if epoch_num > latest_epoch:\n","            latest_epoch = epoch_num\n","            latest_checkpoint = os.path.join(checkpoint_dir, fname)\n","\n","start_epoch =0\n","if latest_checkpoint:\n","    checkpoint = torch.load(latest_checkpoint,map_location=device)\n","\n","    # Restore model and optimizer state:\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # Restore AMP scaler if used\n","    if scaler and checkpoint.get('scaler_state_dict'):\n","        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n","\n","    # Resume from next epoch\n","    start_epoch = checkpoint['epoch']+ 1\n","    print(f\"Restored from {latest_checkpoint}\")\n","else:\n","    print(\" No checkpoint found. Starting from scratch\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzTACjfa_gBS","executionInfo":{"status":"ok","timestamp":1756924052537,"user_tz":-120,"elapsed":254,"user":{"displayName":"GruppoBIxDB","userId":"02526103203990742405"}},"outputId":"18d9337e-36fb-47e6-f411-b32f0e4f000b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" No checkpoint found. Starting from scratch\n"]}]},{"cell_type":"markdown","source":["Training of the model\n","\n"],"metadata":{"id":"jmN8f4xbn55r"}},{"cell_type":"code","source":["\n","## Variables for training progress ##\n","\n","num_epochs = 50    # Total number of epochs\n","num_classes = 19    # Number of segmentation classes\n","\n","# Best mIoU, corresponding checkpoint info and list for storing evaluation of loss function during epochs\n","best_miou = 0.0\n","best_epoch_ckpt = None\n","epoch_loss_list = []\n","\n","\n","## hyperparameters ##\n","initial_lr = 0.025   # Initial learning rate\n","alpha = 0.4     # Weight coefficient for auxiliary loss\n","\n","\n","## training loop ##\n","\n","for epoch in range(start_epoch, num_epochs):\n","    model.train()\n","    total_loss = 0.0\n","\n","    # update learning rate\n","    lr = poly_lr_scheduler(optimizer, initial_lr, epoch)\n","\n","    for batch_idx, (images, labels) in enumerate(train_loader):\n","        images =images.to(device, non_blocking=True)\n","        labels =labels.long().to(device, non_blocking=True)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward step\n","        with torch.cuda.amp.autocast():\n","            outputs = model(images)\n","\n","            if isinstance(outputs, (list, tuple)):\n","                main_pred, *aux_preds =outputs\n","                main_pred =F.interpolate(main_pred, size=labels.shape[1:], mode='bilinear', align_corners=False)\n","\n","                # Main loss\n","                loss = criterion(main_pred, labels)\n","\n","                # Auxiliary losses\n","                for aux in aux_preds:\n","                    if aux is not None:\n","                        aux =F.interpolate(aux, size=labels.shape[1:], mode='bilinear', align_corners=False)\n","                        loss += alpha * criterion(aux, labels)\n","            else:\n","                main_pred = F.interpolate(outputs, size=labels.shape[1:], mode='bilinear', align_corners=False)\n","                loss =criterion(main_pred, labels)\n","\n","\n","        # Backward step\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","        # Cleanup GPU\n","        del images,labels,outputs, loss\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","    epoch_loss = total_loss/len(train_loader)\n","    epoch_loss_list.append(epoch_loss)\n","\n","    ## Miou on validation set ##\n","    all_predictions = []\n","    all_labels = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images =images.to(device, non_blocking=True)\n","            labels =labels.long().to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            if isinstance(outputs, (list, tuple)):\n","                main_pred =outputs[0]\n","            else:\n","                main_pred =outputs\n","\n","            main_pred = F.interpolate(main_pred, size=labels.shape[1:], mode=\"bilinear\", align_corners=False)\n","\n","            probabilities= torch.nn.functional.softmax(main_pred, dim=1)\n","            predictions= torch.argmax(probabilities, dim=1)\n","\n","            all_predictions.append(predictions)\n","            all_labels.append(labels)\n","\n","    all_predictions = torch.cat(all_predictions, dim=0).cpu()\n","    all_labels= torch.cat(all_labels, dim=0).cpu()\n","\n","    epoch_miou, IoU_per_class = compute_miou(all_predictions, all_labels, num_classes=num_classes)\n","    print(f\"End of epoch {epoch+1} — Loss: {epoch_loss:.4f}, mIoU: {epoch_miou:.4f}, LR at the end of the epoch: {lr:.6f}\")\n","\n","\n","    ## Saving checkpoints every 3 epochs ##\n","    if (epoch + 1) % 3 == 0:\n","        checkpoint_filename = f\"checkpoint_epoch{epoch+1}.pt\"\n","        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n","\n","        # Save model, optimizer, scaler, loss and mIoU\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scaler_state_dict': scaler.state_dict(),\n","            'loss': epoch_loss,\n","            'miou': epoch_miou,\n","        }, checkpoint_path)\n","        saved_checkpoints.append(checkpoint_path)\n","        print(f\"Checkpoint saved: {checkpoint_filename}\")\n","\n","        while len(saved_checkpoints) > max_checkpoints:\n","            old_ckpt = saved_checkpoints.popleft()\n","            if os.path.exists(old_ckpt):\n","                os.remove(old_ckpt)\n","                print(f\"Removed old checkpoint: {os.path.basename(old_ckpt)}\")\n","\n","    ## Saving the best model ##\n","    if epoch_miou > best_miou:\n","        best_miou = epoch_miou\n","        best_epoch_ckpt = os.path.join(checkpoint_dir, \"3_best_epoch.pt\")\n","\n","        # Save model, optimizer, scaler, loss and mIoU as the best checkpoin\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scaler_state_dict': scaler.state_dict(),\n","            'loss': epoch_loss,\n","            'miou': epoch_miou,\n","        }, best_epoch_ckpt)\n","        print(f\"New best model saved: {best_epoch_ckpt} con mIoU {best_miou:.4f}\")\n","print(f\"epoch_loss_list: {epoch_loss_list}\")"],"metadata":{"id":"A3tIgiDjcvgX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"D_IBQM5n8OGb"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}