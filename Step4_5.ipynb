{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7cVkgarlQTXg"},"outputs":[],"source":["## Import and setup ##\n","\n","from google.colab import drive\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageFilter\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF\n","from torchvision import models\n","import os\n","import gc\n","import glob\n","import random\n","import collections\n","from collections import deque\n","import contextlib\n","\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","source":["Definition Bisenet model with backbone ResNet\n"],"metadata":{"id":"bcRgFzq3TVYm"}},{"cell_type":"code","source":["## Bisenet model with backbone ResNet18 or ResNet101 ##\n","\n","class resnet18(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet18(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)  # 1 / 4\n","        feature2 = self.layer2(feature1)  # 1 / 8\n","        feature3 = self.layer3(feature2)  # 1 / 16\n","        feature4 = self.layer4(feature3)  # 1 / 32\n","        # global average pooling to build tail\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","class resnet101(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        self.features = models.resnet101(pretrained=pretrained)\n","        self.conv1 = self.features.conv1\n","        self.bn1 = self.features.bn1\n","        self.relu = self.features.relu\n","        self.maxpool1 = self.features.maxpool\n","        self.layer1 = self.features.layer1\n","        self.layer2 = self.features.layer2\n","        self.layer3 = self.features.layer3\n","        self.layer4 = self.features.layer4\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.relu(self.bn1(x))\n","        x = self.maxpool1(x)\n","        feature1 = self.layer1(x)  # 1 / 4\n","        feature2 = self.layer2(feature1)  # 1 / 8\n","        feature3 = self.layer3(feature2)  # 1 / 16\n","        feature4 = self.layer4(feature3)  # 1 / 32\n","        # global average pooling to build tail\n","        tail = torch.mean(feature4, 3, keepdim=True)\n","        tail = torch.mean(tail, 2, keepdim=True)\n","        return feature3, feature4, tail\n","\n","\n","def build_contextpath(name):\n","    model = {\n","        'resnet18': resnet18(pretrained=True),\n","        'resnet101': resnet101(pretrained=True)\n","    }\n","    return model[name]\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n","                               stride=stride, padding=padding, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        return self.relu(self.bn(x))\n","\n","\n","class Spatial_path(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n","        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n","        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n","\n","    def forward(self, input):\n","        x = self.convblock1(input)\n","        x = self.convblock2(x)\n","        x = self.convblock3(x)\n","        return x\n","\n","\n","class AttentionRefinementModule(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.sigmoid = nn.Sigmoid()\n","        self.in_channels = in_channels\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input):\n","        # global average pooling\n","        x = self.avgpool(input)\n","        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n","        x = self.conv(x)\n","        x = self.sigmoid(self.bn(x))\n","        # x = self.sigmoid(x)\n","        # channels of input and x should be same\n","        x = torch.mul(input, x)\n","        return x\n","\n","\n","class FeatureFusionModule(torch.nn.Module):\n","    def __init__(self, num_classes, in_channels):\n","        super().__init__()\n","        # self.in_channels = input_1.channels + input_2.channels\n","        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n","        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n","        self.in_channels = in_channels\n","\n","        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n","        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","    def forward(self, input_1, input_2):\n","        x = torch.cat((input_1, input_2), dim=1)\n","        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n","        feature = self.convblock(x)\n","        x = self.avgpool(feature)\n","\n","        x = self.relu(self.conv1(x))\n","        x = self.sigmoid(self.conv2(x))\n","        x = torch.mul(feature, x)\n","        x = torch.add(x, feature)\n","        return x\n","\n","\n","class BiSeNet(nn.Module):\n","    def __init__(self, num_classes, context_path='resnet18'):\n","        super().__init__()\n","        # build spatial path\n","        self.spatial_path = Spatial_path()\n","\n","        # build spatial path\n","        self.context_path = build_contextpath(context_path)\n","\n","        # build attention refinement module  for resnet 101\n","        if context_path == 'resnet101':\n","            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n","            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(1024, num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(2048, num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n","\n","        elif context_path == 'resnet18':\n","            # build attention refinement module  for resnet 18\n","            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n","            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n","            # supervision block\n","            self.supervision1 = nn.Conv2d(256, num_classes, kernel_size=1)\n","            self.supervision2 = nn.Conv2d(512, num_classes, kernel_size=1)\n","            # build feature fusion module\n","            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n","        else:\n","            print('Error: unspport context_path network \\n')\n","\n","        # build final convolution\n","        self.conv = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n","\n","        self.init_weight()\n","\n","        self.mul_lr = []\n","        self.mul_lr.append(self.spatial_path)\n","        #self.mul_lr.append(self.saptial_path)\n","        self.mul_lr.append(self.attention_refinement_module1)\n","        self.mul_lr.append(self.attention_refinement_module2)\n","        self.mul_lr.append(self.supervision1)\n","        self.mul_lr.append(self.supervision2)\n","        self.mul_lr.append(self.feature_fusion_module)\n","        self.mul_lr.append(self.conv)\n","\n","\n","    def init_weight(self):\n","        for name, m in self.named_modules():\n","            if 'context_path' not in name:\n","                if isinstance(m, nn.Conv2d):\n","                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n","                elif isinstance(m, nn.BatchNorm2d):\n","                    m.eps = 1e-5\n","                    m.momentum = 0.1\n","                    nn.init.constant_(m.weight, 1)\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, input):\n","        # output of spatial path\n","        sx = self.spatial_path(input)\n","\n","        # output of context path\n","        cx1, cx2, tail = self.context_path(input)\n","        cx1 = self.attention_refinement_module1(cx1)\n","        cx2 = self.attention_refinement_module2(cx2)\n","        cx2 = torch.mul(cx2, tail)\n","        # upsampling\n","        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n","        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n","        cx = torch.cat((cx1, cx2), dim=1)\n","\n","        if self.training == True:\n","            cx1_sup = self.supervision1(cx1)\n","            cx2_sup = self.supervision2(cx2)\n","            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n","            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n","\n","        # output of feature fusion module\n","        result = self.feature_fusion_module(sx, cx)\n","\n","        # upsampling\n","        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n","        result = self.conv(result)\n","\n","        if self.training == True:\n","            return result, cx1_sup, cx2_sup\n","\n","        return result\n","\n","def get_bisenet_model(num_classes, context_path='resnet18'):\n","    return BiSeNet(num_classes=num_classes, context_path=context_path)\n"],"metadata":{"id":"Dj-f9h9gQrrX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definition of discriminator, architecture as in article https://openaccess.thecvf.com/content_cvpr_2018/papers/Tsai_Learning_to_Adapt_CVPR_2018_paper.pdf"],"metadata":{"id":"W7TDjb53Tac0"}},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels=19): # Input is a 19-channel class map\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1), # Output a single value\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"4ur16uM_R89M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For EXTENSION: definition of a variations of the cross-entropy loss:  Focal loss"],"metadata":{"id":"VOWhnntbg0RS"}},{"cell_type":"code","source":["class FocalLoss(nn.Module):\n","    def __init__(self, gamma, ignore_index: int = 255, reduction: str = \"mean\"):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.ignore_index = ignore_index\n","        self.reduction = reduction\n","\n","    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n","        # logits = model outputs, target = Ground truth labels\n","\n","        target = target.long()\n","\n","        # Compute log-softmax over the class dimension\n","        log_probs = F.log_softmax(logits, dim=1)  # (N, C, H, W)\n","\n","        # handle ignore_index\n","        if self.ignore_index is not None:\n","            # Create a bool for valid pixels (not equal to ignore_index)\n","            valid = (target != self.ignore_index)\n","            if valid.sum() == 0:\n","                return logits.new_zeros(())\n","            safe_target = target.clone()\n","            safe_target[~valid] = 0\n","        else:\n","            valid = torch.ones_like(target, dtype=torch.bool)\n","            safe_target = target\n","\n","        # Compute focal weight\n","        log_pt = log_probs.gather(1, safe_target.unsqueeze(1)).squeeze(1)\n","        pt = log_pt.exp()\n","        focal_weight = (1 - pt).pow(self.gamma)\n","\n","        # Compute focal loss for each sample\n","        loss = -focal_weight * log_pt\n","        loss = loss[valid]\n","\n","        # Take mean or sum of focal loss\n","        if self.reduction == \"mean\":\n","            return loss.mean()\n","        elif self.reduction == \"sum\":\n","            return loss.sum()\n","        else:\n","            return loss"],"metadata":{"id":"S-vI5U0QgwQ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definition of transformations used as preprocessing and data augmentation"],"metadata":{"id":"3GH_cIp7T1K7"}},{"cell_type":"code","source":["## Class for transformation ##\n","\n","class SegmentationTransform:\n","    \"\"\"\n","    Class to apply preprocessing and data augmentation for image segmentation.\n","    Includes resizing, optional horizontal flipping, color jitter, Gaussian blur,\n","    and normalization to ImageNet statistics.\n","    \"\"\"\n","    def __init__(self, resize, flip = False, color_jitter=False, gaussian_blur=False, train=True):\n","        # Resize for images and masks\n","        self.resize_img = transforms.Resize(resize, interpolation=Image.BILINEAR)\n","        self.resize_mask = transforms.Resize(resize, interpolation=Image.NEAREST)\n","\n","        # Augmentations\n","        self.flip_flag = flip and train\n","        self.color_jitter_flag = color_jitter and train\n","        self.gaussian_blur_flag = gaussian_blur and train\n","        self.color_jitter = transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)\n","\n","        # ImageNet normalization\n","        self.mean = [0.485, 0.456, 0.406]\n","        self.std = [0.229, 0.224, 0.225]\n","\n","    def __call__(self, img, mask):\n","        # Resize\n","        img = self.resize_img(img)\n","        mask = Image.fromarray(mask.astype(np.uint8))\n","        mask = self.resize_mask(mask)\n","\n","        # Horizontal flip\n","        if self.flip_flag and random.random() < 0.5:\n","            img  =TF.hflip(img)\n","            mask =TF.hflip(mask)\n","\n","        # color jitter\n","        if self.color_jitter_flag and random.random() < 0.5:\n","            img = self.color_jitter(img)\n","\n","        # gaussian blur\n","        if self.gaussian_blur_flag and random.random() < 0.5:\n","            img = img.filter(ImageFilter.GaussianBlur(radius=1.5))\n","\n","        # conversion to tensor + normalize\n","        img = TF.to_tensor(img)\n","        img = TF.normalize(img, mean=self.mean, std=self.std)\n","        mask = torch.from_numpy(np.array(mask, dtype=np.uint8)).long()\n","\n","        return img, mask"],"metadata":{"id":"wNPcj9YmRiOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset GTA5"],"metadata":{"id":"6eUv1DSAY83y"}},{"cell_type":"code","source":["ID_TO_TRAINID = {\n","    7:0, 8:1, 11:2, 12:3, 13:4, 17:5,\n","    19:6, 20:7, 21:8, 22:9, 23:10, 24:11,\n","    25:12, 26:13, 27:14, 28:15, 31:16, 32:17, 33:18\n","}     # This dictionary is mapping the original label IDs to the corresponding Cityscapes training IDs.\n","\n","def mapping_labels(label_np):\n","    \"\"\"\n","    Map GTA5 label IDs to Cityscapes training IDs.\n","    ID which is not mapped is set to the ignore index 255.\n","    \"\"\"\n","    mapped =np.full_like(label_np, 255)\n","    for k, v in ID_TO_TRAINID.items():\n","        mapped[label_np == k] =v\n","    return mapped\n","\n","class GTA5Dataset(Dataset):\n","    \"\"\"\n","    Dataset for GTA5 segmentation dataset.\n","    Loads images and labels and applies optional transformations.\n","    \"\"\"\n","    def __init__(self, root, transform=None):\n","        self.transform = transform\n","\n","        # Load all images and labels\n","        self.images =sorted(glob.glob(f\"{root}/images/**/*.png\", recursive=True))\n","        self.labels =sorted(glob.glob(f\"{root}/labels/**/*.png\", recursive=True))\n","        min_len = min(len(self.images), len(self.labels))\n","        self.images = self.images[:min_len]\n","        self.labels = self.labels[:min_len]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","\n","        # Load image and label\n","        img = Image.open(self.images[idx]).convert(\"RGB\")\n","        label_np= np.array(Image.open(self.labels[idx]), dtype=np.int64)\n","\n","        # Map GTA5 labels to Cityscapes training IDs\n","        label_mapped= mapping_labels(label_np)\n","\n","        # Optional transformations\n","        if self.transform:\n","            img, label_mapped = self.transform(img, label_mapped)\n","\n","        return img,label_mapped"],"metadata":{"id":"DqxuggqqRnvt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset Cityscapes"],"metadata":{"id":"gRQZ0oDlY_TN"}},{"cell_type":"code","source":["# Dataset Cityscapes\n","class CityscapesDataset(Dataset):\n","    def __init__(self, root, split=\"val\", transform=None):\n","        self.transform = transform\n","\n","        # Load all images and labels\n","        self.images = sorted(glob.glob(f\"{root}/images/{split}/**/*.png\", recursive=True))\n","        self.labels = sorted(glob.glob(f\"{root}/gtFine/{split}/**/*_labelTrainIds.png\", recursive=True))   # extract the trainIds\n","        min_len = min(len(self.images), len(self.labels))\n","        self.images = self.images[:min_len]\n","        self.labels = self.labels[:min_len]\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","\n","        # Load image and label\n","        img = Image.open(self.images[idx]).convert(\"RGB\")\n","        label_np = np.array(Image.open(self.labels[idx]), dtype=np.int64)\n","\n","        # Optional transformations\n","        if self.transform:\n","            img, label_np = self.transform(img, label_np)\n","\n","        return img, label_np\n","\n","import torch\n"],"metadata":{"id":"DYr8VIwSRqqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transformations for train (taking the data augmentations that perform best in 3b) and validation dataset\n","\n","transform_gta5_best_2_3  = SegmentationTransform(resize=(720,1280), color_jitter=True,  gaussian_blur=True,  train=True)\n","transform_cityscapes_val = SegmentationTransform(resize=(512,1024), train=False)"],"metadata":{"id":"6g68s0K1Rwv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utils Function"],"metadata":{"id":"QjuKxUllZiBB"}},{"cell_type":"code","source":["def compute_miou(preds, labels, num_classes=19, device=\"cuda\"):\n","    \"\"\"\n","    Compute the mean Intersection over Union (mIoU) for semantic segmentation.\n","    This function calculates the IoU for each class and returns both the mean IoU\n","    and the list of per-class IoUs. It handles the common \"void\" label (255) by\n","    excluding it from false positive calculations.\n","    \"\"\"\n","\n","\n","    # Initialize variables for storing True Positives , False Positives and False Negatives\n","    tp = torch.zeros(num_classes, dtype=torch.int64, device=device)\n","    fp = torch.zeros(num_classes, dtype=torch.int64, device=device)\n","    fn = torch.zeros(num_classes, dtype=torch.int64, device=device)\n","\n","\n","    # TP, FP, FN for each class\n","    for cls in range(num_classes):\n","        # True Positive\n","        tp[cls] += ((labels == cls) & (preds == cls)).sum()\n","        # False Positive\n","        fp[cls] += ((labels != cls) & (labels != 255) & (preds == cls)).sum()\n","        # False Negative\n","        fn[cls] += ((labels == cls) & (preds != cls)).sum()\n","\n","    iou_per_class = []\n","\n","    # IoU for each class and store in a list\n","    for cls in range(num_classes):\n","        denom = tp[cls] + fp[cls] + fn[cls]\n","        iou = tp[cls].float() / (denom.float() + 1e-10)\n","        print(f\"Class {cls}: TP={tp[cls].item()}, FP={fp[cls].item()}, FN={fn[cls].item()}, IoU={iou.item():.4f}\")\n","        if denom > 0:  # only include classes with at least one pixel\n","            iou_per_class.append(iou.item())\n","\n","\n","    mean_iou = np.mean(iou_per_class) if iou_per_class else 0.0\n","    return mean_iou, iou_per_class\n","\n","\n","\n","def extract_images(batch):\n","    \"\"\"\n","    Extracts the image tensor from many possible batch format:\n","    - a PyTorch tensor\n","    - a list of tensors\n","    - a dictionary containing tensors\n","    \"\"\"\n","    # Case1: batch is a PyTorch tensor\n","    if torch.is_tensor(batch):\n","        return batch\n","\n","    # Case2: batch is a list or tuple\n","    if isinstance(batch, (list, tuple)):\n","        for item in batch:\n","            if torch.is_tensor(item):\n","                return item\n","        return extract_images(batch[0])\n","\n","    # Case3: batch is a dictionary\n","    if isinstance(batch, dict):\n","        for key in ('image', 'img', 'images', 'pixel_values', 'x'):\n","            if key in batch and torch.is_tensor(batch[key]):\n","                return batch[key]\n","        for v in batch.values():\n","            if torch.is_tensor(v):\n","                return v\n","\n"],"metadata":{"id":"eyZCevtuRzK9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definition of Datasets and Dataloaders for training and validation"],"metadata":{"id":"muUYjdJnPcIn"}},{"cell_type":"code","source":["## Dataset and dataloader ##\n","\n","# Dataset\n","train_dataset = GTA5Dataset(\"/content/drive/MyDrive/GTA5/GTA5_\", transform=transform_gta5_best_2_3)\n","city_dataset = CityscapesDataset(\"/content/drive/MyDrive/Cityscapes/Cityspaces\", split=\"train\", transform=transform_gta5_best_2_3)\n","city_val_dataset = CityscapesDataset(\"/content/drive/MyDrive/Cityscapes/Cityspaces\", split=\"val\", transform=transform_cityscapes_val)\n","\n","# DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n","city_loader = DataLoader(city_dataset, batch_size=4, shuffle=True, num_workers=2)\n","city_val_loader = DataLoader(city_val_dataset, batch_size=8, shuffle=False, num_workers=2)"],"metadata":{"id":"hplrzZHfR1jL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Function for the learning rate: polynomial learning rate"],"metadata":{"id":"JQGPqp-1ZpRs"}},{"cell_type":"code","source":["## learning rate ##\n","\n","def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n","                      max_iter=300, power=0.9):\n","    \"\"\"\n","    Polynomial decay of learning rate\n","        :param init_lr is base learning rate\n","        :param iter is a current iteration\n","        :param lr_decay_iter how frequently decay occurs, default is 1\n","        :param max_iter is number of maximum iterations\n","        :param power is a polymomial power\n","      Returns the scalar learning rate\n","    \"\"\"\n","\n","    lr = init_lr*(1 - iter/max_iter)**power\n","    optimizer.param_groups[0]['lr'] = lr\n","    return lr"],"metadata":{"id":"yni6LcnAR_xu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialization of model, optimizers and loss functions"],"metadata":{"id":"lkW92nDgQhqo"}},{"cell_type":"code","source":["## Model, loss functions and optimizer ##\n","\n","num_classes = 19\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","## hyperparameters\n","initial_lr_s = 2.5e-4\n","initial_lr_d = 1e-4\n","aux_loss_weight = 0.4\n","initial_lr_s = 2.5e-4\n","initial_lr_d = 1e-4\n","lambda_adv = 0.001\n","\n","# Initialize DeepLab v2 model with pre-trained ResNet18 backbone\n","model = get_bisenet_model(num_classes=19, context_path='resnet18')\n","model = model.to(device)\n","\n","# Initialize discriminator\n","discriminator = Discriminator(in_channels=num_classes)\n","discriminator = discriminator.to(device)\n","\n","# Optimizers\n","optimizer_s = torch.optim.SGD(model.parameters(), lr=initial_lr_s, momentum=0.9, weight_decay=1e-4)\n","optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=initial_lr_d,  betas=(0.9, 0.99))\n","\n","#-- FOR EXTENSION --#\n","# optimizer_s = torch.optim.Adam(model.parameters(), lr=initial_lr_s, betas=(0.9, 0.99), weight_decay=1e-4)\n","\n","# Loss functions\n","adv_criterion = nn.BCEWithLogitsLoss().to(device)\n","seg_criterion = nn.CrossEntropyLoss(ignore_index=255).to(device)\n","\n","#-- FOR EXTENSION --#\n","# gammas = 0\n","# gammas = 2\n","# gammas = 3\n","# seg_criterion = FocalLoss(gamma=gammas, ignore_index=255).to(device)"],"metadata":{"id":"XHIJz2dpQiz_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Checkpoint Management, Device Setup, and AMP Initialization.\n","\n","This section prepares the training environment by:\n","   - Creating a directory to save model checkpoints\n","   - Selecting the computation device (GPU if available, else CPU).\n","   - Initializing PyTorch’s Automatic Mixed Precision (AMP) GradScaler"],"metadata":{"id":"nDH44K98mopC"}},{"cell_type":"code","source":["## Setup directories, device, and AMP scaler ##\n","\n","checkpoint_dir = \"/content/drive/MyDrive/4_step_Machine/MachineLearning/checkpoints\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","save_interval = 3\n","max_checkpoints = 2\n","saved_checkpoints = deque(maxlen=max_checkpoints)\n","use_amp = (device.type == 'cuda')\n","\n","\n","# Select device (GPU if available, else CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Automatic Mixed Precision (AMP): GradScaler\n","scaler = torch.amp.GradScaler(device='cuda') if device.type == 'cuda' else None\n","\n","\n","# Management of checkpoints\n","resume_training = True\n","latest_checkpoint = None\n","latest_epoch = -1\n","\n","for fname in os.listdir(checkpoint_dir):\n","    if fname.startswith(\"checkpoint_epoch\") and fname.endswith(\".pt\"):\n","        epoch_num = int(fname.split(\"_epoch\")[1].split(\".\")[0])\n","        if epoch_num > latest_epoch:\n","            latest_epoch = epoch_num\n","            latest_checkpoint = os.path.join(checkpoint_dir, fname)\n","\n","start_epoch = 0\n","if latest_checkpoint:\n","    checkpoint = torch.load(latest_checkpoint, map_location=device)\n","\n","    # Restore model and optimizer state:\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # Restore AMP scaler if used\n","    if scaler and checkpoint.get('scaler_state_dict'):\n","        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n","\n","    # Resume from next epoch\n","    start_epoch = checkpoint['epoch'] + 1\n","    print(f\"Restored from {latest_checkpoint}\")\n","else:\n","    print(\" No checkpoint found. Starting from scratch\")"],"metadata":{"id":"VT_HWOSHSJJx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756927741095,"user_tz":-120,"elapsed":15,"user":{"displayName":"GruppoBIxDB","userId":"02526103203990742405"}},"outputId":"0636b838-44a6-407d-d543-9f51ba78d22d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" No checkpoint found. Starting from scratch\n"]}]},{"cell_type":"markdown","source":["Training of model:\n","this section sets up an Adversarial training loop for domain adaptation."],"metadata":{"id":"a3XDt1oCnc1d"}},{"cell_type":"code","source":["\n","## Variables for training progress ##\n","num_epochs = 50\n","num_classes = 19\n","save_interval = 3\n","global_iter = 0\n","adv_epochs = 3\n","\n","# Best mIoU, corresponding checkpoint info and list for storing evaluation of loss function during epochs\n","best_miou = 0.0\n","best_epoch_ckpt = None\n","epoch_loss_list = []\n","\n","\n","## training loop ##\n","\n","for epoch in range(start_epoch, num_epochs):\n","\n","    model.train()\n","    discriminator.train()\n","\n","    total_loss = 0.0\n","\n","    # update learning rate\n","    lr = poly_lr_scheduler(optimizer_s, initial_lr_s, epoch)\n","\n","    target_iter = iter(city_loader)\n","\n","    for batch_idx, (images_s, labels_s) in enumerate(train_loader):\n","        global_iter += 1\n","\n","        images_s =images_s.to(device, non_blocking=True)\n","        labels_s =labels_s.long().to(device, non_blocking=True)\n","\n","        optimizer_s.zero_grad()\n","        optimizer_d.zero_grad()\n","\n","        # Extract a batch from dataset cityscapes (target domain)\n","        try:\n","            batch_t = next(target_iter)\n","        except StopIteration:\n","            target_iter = iter(city_loader)\n","            batch_t = next(target_iter)\n","\n","        images_t = extract_images(batch_t).to(device, non_blocking=True)\n","\n","        with torch.autocast(device_type='cuda', enabled=use_amp):\n","\n","            ## (1) Forward pass on source (with labels) and target (without labels)\n","            pred_s = model(images_s)\n","            pred_t = model(images_t)\n","\n","            if isinstance(pred_s, (list, tuple)):\n","                main_pred_s, *aux_preds_s = pred_s\n","                main_pred_t, *aux_preds_t= pred_t\n","\n","                # Main loss\n","                loss_seg = seg_criterion(main_pred_s, labels_s)\n","\n","                # Auxiliary losses\n","                for aux_s in aux_preds_s:\n","                    loss_seg += aux_loss_weight*seg_criterion(aux_s, labels_s)\n","\n","            else:\n","                main_pred_s = pred_s\n","                main_pred_t = pred_t\n","                loss_seg =seg_criterion(main_pred_s, labels_s)\n","\n","            ## (2) Train discriminator: predict domain from segmentation outputs\n","            pred_s_prob = F.softmax(main_pred_s, dim=1)\n","            pred_t_prob = F.softmax(main_pred_t, dim=1)\n","\n","            pred_s_prob = F.interpolate(pred_s_prob, size=images_s.shape[2:], mode=\"bilinear\",align_corners=False)\n","            pred_t_prob =F.interpolate(pred_t_prob, size=images_t.shape[2:], mode=\"bilinear\",align_corners=False)\n","\n","            # Discriminator predictions\n","            d_out_s = discriminator(pred_s_prob.detach())\n","            d_out_t = discriminator(pred_t_prob.detach())\n","\n","            # labels for discriminator\n","            d_label_s = torch.ones_like(d_out_s)   # source - real\n","            d_label_t =torch.zeros_like(d_out_t)  # target - fake\n","\n","            # Discriminator loss\n","            loss_adv_d = adv_criterion(d_out_s, d_label_s) + adv_criterion(d_out_t, d_label_t)\n","\n","            ## (3) Train generator so that discriminator fail to classifie\n","            d_out_t_for_gen = discriminator(pred_t_prob)\n","            loss_adv_gen = adv_criterion(d_out_t_for_gen, torch.ones_like(d_out_t_for_gen))\n","\n","            ## (4) Total generator loss = supervised loss + adversarial loss\n","            loss_g =loss_seg + lambda_adv * loss_adv_gen\n","\n","        # Update generator\n","        optimizer_s.zero_grad(set_to_none=True)\n","        scaler.scale(loss_g).backward()\n","        scaler.step(optimizer_s)\n","        scaler.update()\n","\n","        # Update discriminator\n","        optimizer_d.zero_grad(set_to_none=True)\n","        scaler.scale(loss_adv_d).backward()\n","        scaler.step(optimizer_d)\n","        scaler.update()\n","\n","        total_loss+=loss_g.item()\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    epoch_loss = total_loss /len(train_loader)\n","    epoch_loss_list.append(epoch_loss)\n","\n","    ## Miou on validation set ##\n","\n","    all_predictions = []\n","    all_labels = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for images, labels in city_val_loader:\n","            images =images.to(device, non_blocking=True)\n","            labels =labels.long().to(device, non_blocking=True)\n","\n","            outputs = model(images)\n","            main_pred = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n","\n","            main_pred =F.interpolate(main_pred, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n","            probabilities =F.softmax(main_pred, dim=1)\n","            predictions = torch.argmax(probabilities, dim=1)\n","\n","            all_predictions.append(predictions)\n","            all_labels.append(labels)\n","\n","    all_predictions = torch.cat(all_predictions, dim=0).cpu()\n","    all_labels =torch.cat(all_labels, dim=0).cpu()\n","\n","    epoch_miou, IoU_per_class = compute_miou(all_predictions, all_labels, num_classes=num_classes)\n","    print(f\"End of epoch {epoch+1} — Loss: {epoch_loss:.4f}, mIoU: {epoch_miou:.4f}, LR of the final epoch: {lr:.6f}\")\n","\n","    ## Saving checkpoints every 3 epochs ##\n","    if (epoch + 1) % save_interval == 0:\n","        checkpoint_filename = f\"checkpoint_epoch{epoch+1}.pt\"\n","        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_s_state_dict': optimizer_s.state_dict(),\n","            'optimizer_d_state_dict': optimizer_d.state_dict(),\n","            'scaler_state_dict': scaler.state_dict(),\n","            'loss': epoch_loss,\n","            'miou': epoch_miou,\n","        }, checkpoint_path)\n","        saved_checkpoints.append(checkpoint_path)\n","        print(f\"Checkpoint saved: {checkpoint_filename}\")\n","\n","        while len(saved_checkpoints) > max_checkpoints:\n","            old_ckpt = saved_checkpoints.popleft()\n","            if os.path.exists(old_ckpt):\n","                os.remove(old_ckpt)\n","                print(f\"Removed old checkpoint: {os.path.basename(old_ckpt)}\")\n","\n","    ## Saving the best model ##\n","    if epoch_miou > best_miou:\n","        best_miou = epoch_miou\n","        best_epoch_ckpt = os.path.join(checkpoint_dir, \"4_step_best_epoch.pt\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_s_state_dict': optimizer_s.state_dict(),\n","            'optimizer_d_state_dict': optimizer_d.state_dict(),\n","            'scaler_state_dict': scaler.state_dict(),\n","            'loss': epoch_loss,\n","            'miou': epoch_miou,\n","        }, best_epoch_ckpt)\n","        print(f\"New best model saved: {best_epoch_ckpt} con mIoU {best_miou:.4f}\")\n","\n","print(f\"epoch_loss_list: {epoch_loss_list}\")\n","\n"],"metadata":{"id":"WeqS0OwpSLfS"},"execution_count":null,"outputs":[]}]}